{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abfrage der SRU-Schnittstelle von Alma\n",
    "Code aus dem Library Carpentry Workshop der UB Basel vom 12.4.2021-13.04.2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vorgehen und Ziel\n",
    "\n",
    "Das Ziel von diesem Projekt war es, anhand von MMS-ID (Systemnummern) aus Alma die SRU-Schnittstelle der IZ Basel nach den dazugehörigen bibliografischen Daten abzufragen. Zudem soll ausgewählt werden können, welche MARC-Felder ausgegeben werden sollen und alles soll statt im XML-Format in Tabellenform ausgegeben werden.\n",
    "\n",
    "Was ist SRU? <br>\n",
    "https://de.wikipedia.org/wiki/Search/Retrieve_via_URL <br>\n",
    "https://slsp.ch/de/metadata <br>\n",
    "https://witzigs.gitlab.io/cas-dmit-metadaten/daten_beziehen/sru/requests/ <br>\n",
    "SRU-Schnittstelle IZ Basel: https://slsp-network.alma.exlibrisgroup.com/view/sru/41SLSP_UBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Für das Vorgehen werden einige Bibliotheken benötigt, die in Python importiert werden müssen \n",
    "\n",
    "#urllib.request --> braucht es um die \"SRU-URLs\" gut lesbar abzubilden.\n",
    "import urllib.request\n",
    "#pandas as pd --> pandas ist eine Bibliothek, die hilft Daten zu verwalten. Hier wird sie z.B. gebraucht um Excel auszulesen. \n",
    "import pandas as pd\n",
    "#time --> time wird hier benötigt, um die Arbeitsschritte zwischenzeitlich zu pausieren, damit die SRU-Schnittstelle nicht zu viele Anfragen auf einmal bearbeiten muss. \n",
    "import time\n",
    "#bs4 BeautifulSoup --> Beautfulsoup hilft, die MARC-XML in CSV umzuwandeln. \n",
    "from bs4 import BeautifulSoup\n",
    "#csv --> Die csv-Bibliothek hilft, die csv-Datei zu erstellen. \n",
    "import csv\n",
    "\n",
    "\n",
    "#Als ersters wird die benötigte Excel-Datei mit den MMS-IDs ausgelesen. Wichtig ist, dass das Excelfile im gleichen Ordner wie das Jupyter-Notebook oder das py-file gespeichert ist.\n",
    "df = pd.read_excel ('DOKSF_NOT_DOKSDIZS_NOT_DOKSEZS.xlsx')\n",
    "\n",
    "#In einem weiteren Arbeitsschritt wird die benötigte Excelspalte ausgelesen. (In Anführungszeichen in der eckigen Klammer.)\n",
    "#Die IDs werden ausgelesen und mit dem Befehlt \"tolist\" in einer Liste gespeichert.\n",
    "id_list = df['MMS-ID'].tolist()\n",
    "\n",
    "#Mit den IDs kann nun für jede Aufnahme ein Link auf die SRU-Schnittstelle erstellt werden. Der Link ist immer gleich aufgebaut, die MMS-ID soll am Schluss eingefügt werden.\n",
    "sru_anfang = 'https://slsp-network.alma.exlibrisgroup.com/view/sru/41SLSP_UBS?version=1.2&operation=searchRetrieve&recordSchema=marcxml&query=alma.mms_id=='\n",
    "\n",
    "#Ein csv \"f\" wird im Schreibmodus geöffnet. Achtung, dass encoding UTF-8 macht danach allenfalls in Excel Probleme und muss manuell auf UTF-16 gesetzt werden.\n",
    "f = csv.writer(open(\"daten_dump_DOKSF_NOT_DOKSDIZS_NOT_DOKSEZS.csv\", \"w\", encoding=\"UTF-8\"))\n",
    "\n",
    "#Hier werden die gewünschten Bezeichnungen der Spalten definiert. In Anführungszeichen und mit Komma abgetrennt.\n",
    "f.writerow([\"001\", \"110 a\", \"110 0\", \"710 a\", \"710 0\", \"520 a\", \"245 a\", \"245 p\", \"856 u\", \"990 f\", \"008\", \"264 c\"])\n",
    "\n",
    "\n",
    "\n",
    "#Hier werden Funktionen erstellt, um die benötigsten Infos aus den MARC-Feldern zu holen.\n",
    "#Es gibt unglaubliche viele Arten von MARC-Feldern (mit und ohne Unterfelder, wiederholbar nicht wiederholbar etc.)\n",
    "#Hier wurden für zwei Arten Funktionen erstellt:\n",
    "#1. Unwiederholbare Felder ohne Unterfeld (z.B. 001 mit MMS-ID)\n",
    "#2. Felder, die mehrmals vorkommen können. Die Unterfelder sind aber pro Feld einmalig. (z.B. Schlagwörter in 6XX-Felder)\n",
    "#Mit diesem Script sind also nicht alle Arten von Feldern abgedeckt. Mit einer weiteren Funktion kann dies aber hier ergänzt werden.\n",
    "\n",
    "\n",
    "def unwiederholbare_ohne_unterfeld(feld_tag):\n",
    "    #Bei beiden Feldern wurde try- und except eingefügt, weil ansonsten häufig aufgrund eines fehlerhaften Datensatzes das ganze Script abgebrochen wird. Aber daher ist auch empfehlenswert zu prüfen, ob wirklich alle Datensätze in mein CSV übernommen wurden.\n",
    "    try:\n",
    "        unwiederholbares_feld = soup.find(tag=feld_tag).get_text()\n",
    "        return(unwiederholbares_feld)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "def unterfeldurchgehen(feld_tag, unterfeld_code):\n",
    "    try:\n",
    "        feldname_list = [unterfeld.find(code=unterfeld_code).get_text() \n",
    "                         for unterfeld in soup.find_all(tag=feld_tag)]\n",
    "        #wenn es mehere MARC-Feldern mit den gleichen Unterfeldern gibt, werden die Unterfelder mit einem | abgetrennt\n",
    "        #weil es bei der Verarbeitung von csv in Excel immer Probleme gibt, wurden hier ; zu / umgewandelt. Dies ist aber fakultativ.\n",
    "        beautiful_string = \"| \".join(feldname_list).replace(\";\", \"/\")\n",
    "        return beautiful_string\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "#Hier beginnt die eigentliche Programmschleife. Die einzelnen IDs aus der Excelliste werden laufend zum SRU-String zusammengefügt.\n",
    "#Mit Hilfe der Bibliothek \"urllib\" werden die einzelnen Links ausgelesen.\n",
    "for cur_id in id_list:\n",
    "    sru_strings= sru_anfang + str(cur_id)\n",
    "    \n",
    "    sru_data = urllib.request.urlopen(sru_strings).read()\n",
    "   \n",
    " #Damit die SRU-Schnittselle nicht überfodert wird, werden die Abfragen nur im 2-Sekunden-Takt ausgeführt.   \n",
    "    time.sleep(2)\n",
    "#Mit Hilfe der Bibliothek \"BeautifulSoup\" werden die erhaltenen Daten als XML erkannt und können ausgelesen werden.\n",
    "    soup = BeautifulSoup(sru_data, 'xml')\n",
    "\n",
    "    \n",
    " #Die letzte Zeile kann nach Bedarf angepasst werden. Hier werden nämlich die MARC-Felder definiert, die in der CSV-Datei auglesen werden sollen.\n",
    "#Dafür werden die oben definierten Funktionen ausgeführt. Wichtig ist, dass die ausgewählten MARC-Felder mit den oben definierten Spaltennamen übereinstimmten.\n",
    "    f.writerow([ unwiederholbare_ohne_unterfeld(\"001\"), unterfeldurchgehen(\"110\", \"a\"), unterfeldurchgehen(\"110\", \"0\"), unterfeldurchgehen(\"710\", \"a\"), unterfeldurchgehen(\"710\", \"0\"), unterfeldurchgehen(\"520\", \"a\"), unterfeldurchgehen(\"245\", \"a\"),  unterfeldurchgehen(\"245\", \"p\"), unterfeldurchgehen(\"856\", \"u\"), unterfeldurchgehen(\"990\", \"f\"),  unwiederholbare_ohne_unterfeld(\"008\") , unterfeldurchgehen(\"264\", \"c\") ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternatives Vorgehen\n",
    "\n",
    "Das obenstehende Script beschreibt ein Vorgehen, dass mit bereits bekannten Metadaten in einer Excelliste arbeitet.\n",
    "Natürlich ist es auch möglich, direkt eine SRU-Abfrage zu formuliren und danach die so gewonnen Metadaten genau gleich als CSV auszudrucken.\n",
    "\n",
    "Beispiel:\n",
    "Suche nach Noten vor 1600:\n",
    "https://swisscovery.slsp.ch/view/sru/41SLSP_UBS?version=1.2&operation=searchRetrieve&recordSchema=marcxml&query=main_pub_date<=1600%20and%20alma.content_type_code=ntm\n",
    "\n",
    "\n",
    "Folgendes ist dabei unbedingt zu beachten: die SRU-Schnittstelle gibt höchstens 10 Ergebnisse zurück. Um weitere Ergebnisse zu erhalten muss der Link mit \"&startRecord=11\" usw. ergänzt werden.\n",
    "\n",
    "Dazu gibt es auch sehr spannende Tutorials bei der DNB:\n",
    "https://www.dnb.de/DE/Professionell/Services/WissenschaftundForschung/DNBLab/dnblab_node.html#doc731014bodyText3\n",
    "https://hub.gke2.mybinder.org/user/deutsche-nation-bliothek-dnblab-7mydyelo/notebooks/DNB_SRU_Tutorial.ipynb\n",
    "Man kann die Tutorials der DNB auch gleich als Jupyter Notebook laden, was wirklich super ist!\n",
    "\n",
    "\n",
    "##### Vielen Dank an das \"Team 4\" (Iris, Christina, Johanna) für den Input dazu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Für das Vorgehen werden einige Bibliotheken benötigt, die in Python importiert werden müssen \n",
    "\n",
    "#urllib.request --> braucht es um die \"SRU-URLs\" gut lesbar abzubilden.\n",
    "import urllib.request\n",
    "#pandas as pd --> pandas ist eine Bibliothek, die hilft Daten zu verwalten. Hier wird sie z.B. gebraucht um Excel auszulesen. \n",
    "import pandas as pd\n",
    "#time --> time wird hier benötigt, um die Arbeitsschritte zwischenzeitlich zu pausieren, damit die SRU-Schnittstelle nicht zu viele Anfragen auf einmal bearbeiten muss. \n",
    "import time\n",
    "#bs4 BeautifulSoup --> Beautfulsoup hilft, die MARC-XML in CSV umzuwandeln. \n",
    "from bs4 import BeautifulSoup\n",
    "#csv --> Die csv-Bibliothek hilft, die csv-Datei zu erstellen. \n",
    "import csv\n",
    "\n",
    "\n",
    "#Die vorher formulierte Abfrage wird als \"Basis-Url\" gespeichert.\n",
    "sru_basis = \"https://swisscovery.slsp.ch/view/sru/41SLSP_UBS?version=1.2&operation=searchRetrieve&recordSchema=marcxml&query=main_pub_date<=1600%20and%20alma.content_type_code=ntm\"\n",
    "\n",
    "#Die Basis-URL wird geöffneet und ausgelesen.\n",
    "sru_data = urllib.request.urlopen(sru_basis).read()\n",
    "\n",
    "#Mit Hilfe der Bibliothek \"BeautifulSoup\" werden die erhaltenen Daten als XML erkannt und können ausgelesen werden.\n",
    "\n",
    "soup = BeautifulSoup(sru_data, 'xml')\n",
    "\n",
    "#Es wird eine Variable erstellt mit der Gesamtanzahl der Records, welche die Abfrage enthaltet.\n",
    "num_records = int(soup.find(\"numberOfRecords\").text)\n",
    "\n",
    "\n",
    "#Mit Hilfe einer Vorschleife wird der Zusatz \"&startRecord=\" erstellt. Es wird für jeden einzelnen Record einen Link mit diesem Zusatz erstellt und in eine Liste gespeichert.\n",
    "\n",
    "liste_paramater_links = []\n",
    "\n",
    "\n",
    "for parameter in range(num_records):\n",
    "\n",
    "     if parameter < num_records:\n",
    "\n",
    "        liste_paramater_links.append(\"&startRecord=\"+str(parameter+1))\n",
    "\n",
    "#Ein csv \"f\" wird im Schreibmodus geöffnet. Achtung, dass encoding UTF-8 macht danach allenfalls in Excel Probleme und muss manuell auf UTF-16 gesetzt werden.\n",
    "f = csv.writer(open(\"ntm_vor_1600.csv\", \"w\", encoding=\"UTF-8\"))\n",
    "\n",
    "#Hier werden die gewünschten Bezeichnungen der Spalten definiert. In Anführungszeichen und mit Komma abgetrennt.\n",
    "f.writerow([\"001\", \"110 a\", \"110 0\", \"710 a\", \"710 0\", \"520 a\", \"245 a\", \"245 p\", \"856 u\", \"990 f\", \"008\", \"264 c\"])\n",
    "\n",
    "\n",
    "\n",
    "#Hier werden Funktionen erstellt, um die benötigsten Infos aus den MARC-Feldern zu holen.\n",
    "#Es gibt unglaubliche viele Arten von MARC-Feldern (mit und ohne Unterfelder, wiederholbar nicht wiederholbar etc.)\n",
    "#Hier wurden für zwei Arten Funktionen erstellt:\n",
    "#1. Unwiederholbare Felder ohne Unterfeld (z.B. 001 mit MMS-ID)\n",
    "#2. Felder, die mehrmals vorkommen können. Die Unterfelder sind aber pro Feld einmalig. (z.B. Schlagwörter in 6XX-Felder)\n",
    "#Mit diesem Script sind also nicht alle Arten von Feldern abgedeckt. Mit einer weiteren Funktion kann dies aber hier ergänzt werden.\n",
    "\n",
    "\n",
    "def unwiederholbare_ohne_unterfeld(feld_tag):\n",
    "    #Bei beiden Feldern wurde try- und except eingefügt, weil ansonsten häufig aufgrund eines fehlerhaften Datensatzes das ganze Script abgebrochen wird. Aber daher ist auch empfehlenswert zu prüfen, ob wirklich alle Datensätze in mein CSV übernommen wurden.\n",
    "    try:\n",
    "        unwiederholbares_feld = soup_alles.find(tag=feld_tag).get_text()\n",
    "        return(unwiederholbares_feld)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "def unterfeldurchgehen(feld_tag, unterfeld_code):\n",
    "    try:\n",
    "        feldname_list = [unterfeld.find(code=unterfeld_code).get_text() \n",
    "                         for unterfeld in soup_alles.find_all(tag=feld_tag)]\n",
    "        #wenn es mehere MARC-Feldern mit den gleichen Unterfeldern gibt, werden die Unterfelder mit einem | abgetrennt\n",
    "        #weil es bei der Verarbeitung von csv in Excel immer Probleme gibt, wurden hier ; zu / umgewandelt. Dies ist aber fakultativ.\n",
    "        beautiful_string = \"| \".join(feldname_list).replace(\";\", \"/\")\n",
    "        return beautiful_string\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "#Hier beginnt die eigentliche Programmschleife. Die einzelnen URLs mit dem Zusatz werden laufend zum SRU-String zusammengefügt.\n",
    "  \n",
    "for cur_parameter in liste_paramater_links:\n",
    "    sru_strings= sru_basis + str(cur_parameter)\n",
    " \n",
    "    sru_data_alles = urllib.request.urlopen(sru_strings).read()\n",
    "   \n",
    " #Damit die SRU-Schnittselle nicht überfodert wird, werden die Abfragen nur im 2-Sekunden-Takt ausgeführt.   \n",
    "    time.sleep(2)\n",
    "#Mit Hilfe der Bibliothek \"BeautifulSoup\" werden die erhaltenen Daten als XML erkannt und können ausgelesen werden.\n",
    "    soup_alles = BeautifulSoup(sru_data_alles, 'xml')\n",
    "    \n",
    "   \n",
    " #Die letzte Zeile kann nach Bedarf angepasst werden. Hier werden nämlich die MARC-Felder definiert, die in der CSV-Datei auglesen werden sollen.\n",
    "#Dafür werden die oben definierten Funktionen ausgeführt. Wichtig ist, dass die ausgewählten MARC-Felder mit den oben definierten Spaltennamen übereinstimmten.\n",
    "    f.writerow([ unwiederholbare_ohne_unterfeld(\"001\"), unterfeldurchgehen(\"110\", \"a\"), unterfeldurchgehen(\"110\", \"0\"), unterfeldurchgehen(\"710\", \"a\"), unterfeldurchgehen(\"710\", \"0\"), unterfeldurchgehen(\"520\", \"a\"), unterfeldurchgehen(\"245\", \"a\"),  unterfeldurchgehen(\"245\", \"p\"), unterfeldurchgehen(\"856\", \"u\"), unterfeldurchgehen(\"990\", \"f\"),  unwiederholbare_ohne_unterfeld(\"008\") , unterfeldurchgehen(\"264\", \"c\") ])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispiel um die erhaltenen CSV-Dateien miteinander abzugleichen\n",
    "\n",
    "Dabei ist die Bibliothek \"Pandas\" und deren Befehl \"merge\" sehr nützlich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bibliotheken importieren\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "#CSV-Dateien öffnen und eine Variable als Titel vergeben. \n",
    "#Achtung: Die CSV-Dateien müssen im gleichen Ordner wie das Jupyter-Notebook oder das py-file gespeichert sein.\n",
    "dizas = pd.read_csv(\"daten_dump_doksdizs.csv\",sep=\",\")\n",
    "ezas = pd.read_csv(\"daten_dump_doksezs.csv\",sep=\",\")\n",
    "\n",
    "#mergen von zwei verschiedenen Tabellen ahnad von Spaltennamen (im Beispiel: 110 0). \n",
    "#Der Befehlt pd.notnull macht, dass leere Spalten nicht \"gemergt\" werden.\n",
    "\n",
    "ezas.merge(dizas[pd.notnull(dizas['110 0'])], on='110 0')\n",
    "\n",
    "#gemergete Tabelle wieder als csv-abspeichern\n",
    "\n",
    "gemergtes_csv = ezas.merge(dizas[pd.notnull(dizas['110 0'])], on='110 0')\n",
    "\n",
    "gemergtes_csv.to_csv( \"combined_csv.csv\", encoding='utf-16')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
